{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¥ Costream: Time-Series Anomaly Detection Demo (ECG)\n",
    "\n",
    "This notebook demonstrates how to use the **`costream`** package for **Univariate Anomaly Detection**.\n",
    "\n",
    "While `costream` was designed for fall detection, its core logic (Cost-Sensitive Classification + Streaming Segmentation) applies perfectly to detecting medical anomalies, such as heart arrhythmias.\n",
    "\n",
    "**Dataset**: `ECGDiffCount3` (via `aeon`)\n",
    "**Goal**: Detect abnormal heartbeats in a continuous stream, prioritizing Recall (don't miss an event)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# --- Costream Imports ---\n",
    "from costream.segmentation.training_segmenter import create_training_data\n",
    "from costream.model.cost_classifier_cv import CostClassifierCV\n",
    "from costream.evaluation.tester import run_experiment, ModelSpec\n",
    "from costream.evaluation.visualization import plot_confidence, set_style\n",
    "\n",
    "set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading (Aeon)\n",
    "\n",
    "We load the `ECGDiffCount3` dataset. We ensure it is flattened to 1D (Univariate) and labels are mapped to `0` (Normal) vs `1` (Anomaly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Try the specific loader requested\n",
    "    from aeon.datasets import load_ecg_diff_count_3\n",
    "    # Note: Adjust unpacking based on actual return signature of your specific loader version\n",
    "    # Assuming: X, y split or single return. We use a generic fallback below if this varies.\n",
    "    X_test, y_test, X_train, y_train = load_ecg_diff_count_3(\"supervised\")\n",
    "except ImportError:\n",
    "    # Fallback to standard AEON API if specific function name differs\n",
    "    print(\"âš ï¸ Specific loader not found, using generic load_classification...\")\n",
    "    from aeon.datasets import load_classification\n",
    "    X_train, y_train = load_classification(\"ECGDiffCount3\", split=\"train\")\n",
    "    X_test, y_test = load_classification(\"ECGDiffCount3\", split=\"test\")\n",
    "\n",
    "# 1. Adapt Dimensions: (N, 1, T) -> (N, T)\n",
    "if X_train.ndim == 3:\n",
    "    X_train = X_train.squeeze()\n",
    "if X_test.ndim == 3:\n",
    "    X_test = X_test.squeeze()\n",
    "\n",
    "# 2. Adapt Labels to Binary (0=Normal, 1=Anomaly)\n",
    "# We assume the minority class is the Anomaly. Let's check counts.\n",
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "minority_class = classes[np.argmin(counts)]\n",
    "\n",
    "print(f\"Original Classes: {classes}, Counts: {counts}\")\n",
    "print(f\"Treating Class '{minority_class}' as Anomaly (1)\")\n",
    "\n",
    "y_train_bin = (y_train == minority_class).astype(int)\n",
    "y_test_bin = (y_test == minority_class).astype(int)\n",
    "\n",
    "print(f\"Train Shape: {X_train.shape} | Anomalies: {sum(y_train_bin)}\")\n",
    "print(f\"Test Shape:  {X_test.shape} | Anomalies: {sum(y_test_bin)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare for Costream\n",
    "\n",
    "`Costream` data loaders typically expect a **List of DataFrames**, where each DataFrame represents one recording (one subject/trial). \n",
    "\n",
    "We will convert the numpy arrays into this format to utilize the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataframe_list(X, y):\n",
    "    dfs = []\n",
    "    for signal, label in zip(X, y):\n",
    "        # Create DF with 'ecg' column\n",
    "        df = pd.DataFrame(signal, columns=['ecg'])\n",
    "        # The label applies to the whole segment in this dataset context,\n",
    "        # but for the segmenter, we mark the 'event' at the center if it's an anomaly.\n",
    "        df['label'] = 0\n",
    "        if label == 1:\n",
    "            mid_point = len(signal) // 2\n",
    "            df.loc[mid_point, 'label'] = 1\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "train_dfs = to_dataframe_list(X_train, y_train_bin)\n",
    "\n",
    "# Visualize one Anomaly\n",
    "anomaly_idx = np.where(y_train_bin == 1)[0][0]\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(train_dfs[anomaly_idx]['ecg'], label=\"Signal\")\n",
    "plt.title(f\"Example Anomaly (Train Idx {anomaly_idx})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Segmentation\n",
    "\n",
    "We segment the signals into windows. Since this dataset is already short segments (~100-2000 points), we will set a small window size relative to the signal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine appropriate window size based on dataset length\n",
    "sig_len = X_train.shape[1]\n",
    "FREQ = 100 # Assumed Hz for this demo\n",
    "win_sec = (sig_len / FREQ) * 0.5 # Window = 50% of signal length\n",
    "\n",
    "print(f\"Signal Len: {sig_len}, Using Window Size: {win_sec:.2f}s\")\n",
    "\n",
    "X_train_seg, y_train_seg = create_training_data(\n",
    "    train_dfs,\n",
    "    feature_cols=['ecg'],\n",
    "    window_size=win_sec,\n",
    "    step=win_sec / 2, # 50% overlap\n",
    "    freq=FREQ,\n",
    "    activity_threshold=0.0, # Keep all windows (don't filter quiet parts for ECG)\n",
    "    spacing=\"na\" # Center the event\n",
    ")\n",
    "\n",
    "# Flatten 3D (N, Win, 1) to 2D (N, Win) for sklearn\n",
    "if X_train_seg.ndim == 3:\n",
    "    X_train_seg = X_train_seg[:, :, 0]\n",
    "\n",
    "print(f\"Segmented Shape: {X_train_seg.shape}\")\n",
    "print(f\"Class Balance: {np.bincount(y_train_seg)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Cost-Sensitive Model\n",
    "\n",
    "We use **`CostClassifierCV`** to penalize missing an anomaly (False Negative) significantly more than raising a False Alarm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CostClassifierCV(\n",
    "    base_estimators=[\n",
    "        LogisticRegression(max_iter=500, solver='liblinear'),\n",
    "        RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42)\n",
    "    ],\n",
    "    alpha=3.0,          # 1 Missed Anomaly = 3 False Alarms cost-wise\n",
    "    method=\"stacking\",  # Use meta-learner to fuse probs\n",
    "    cv=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"ðŸ§  Training Model...\")\n",
    "model.fit(X_train_seg, y_train_seg)\n",
    "print(f\"Done. Threshold: {model.threshold_:.3f}, Weights: {model.weights_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Streaming Evaluation\n",
    "\n",
    "To test the \"streaming\" capabilities, we will stitch together the **Test Set** samples into continuous long streams. This simulates a real monitoring scenario where anomalies occur sporadically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 synthetic streams by concatenating test samples\n",
    "n_streams = 3\n",
    "samples_per_stream = 20\n",
    "\n",
    "test_signals = []\n",
    "test_events = []\n",
    "\n",
    "for i in range(n_streams):\n",
    "    # Select random samples from test set\n",
    "    indices = np.random.choice(len(X_test), samples_per_stream, replace=False)\n",
    "    \n",
    "    # Stitch signal\n",
    "    stream = np.concatenate(X_test[indices])\n",
    "    \n",
    "    # Calculate where the anomalies are in this new stream\n",
    "    current_idx = 0\n",
    "    events = []\n",
    "    for idx in indices:\n",
    "        if y_test_bin[idx] == 1:\n",
    "            # Anomaly is at the center of this segment\n",
    "            events.append(current_idx + (sig_len // 2))\n",
    "        current_idx += sig_len\n",
    "        \n",
    "    test_signals.append(stream)\n",
    "    test_events.append(events if events else -1)\n",
    "\n",
    "print(f\"Generated {len(test_signals)} streams of length {len(test_signals[0])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Evaluation Engine\n",
    "results = run_experiment(\n",
    "    X_train_seg, y_train_seg,\n",
    "    test_signals=test_signals,\n",
    "    test_event_points=test_events,\n",
    "    model_specs=[ModelSpec(\"Cost_Ensemble\", model)],\n",
    "    window_size=win_sec,\n",
    "    step=win_sec / 2,\n",
    "    freq=FREQ,\n",
    "    signal_thresh=0.0, # Process all windows\n",
    "    tolerance=win_sec, # Allow detection within 1 window length\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Let's look at one of the stitched streams to see the model's confidence map reacting to the anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a stream that definitely has events\n",
    "stream_idx = next(i for i, ev in enumerate(test_events) if ev != -1)\n",
    "\n",
    "signal = test_signals[stream_idx]\n",
    "events = test_events[stream_idx]\n",
    "\n",
    "# Re-run inference just for this plot (or grab from cache if extended)\n",
    "from costream.segmentation.streaming_segmenter import sliding_window_inference\n",
    "from costream.evaluation.event_detection import evaluate_recording\n",
    "\n",
    "conf_map, _ = sliding_window_inference(\n",
    "    signal, model, \n",
    "    window_size=win_sec, step=win_sec/2, freq=FREQ, signal_thresh=0.0\n",
    ")\n",
    "\n",
    "# Calculate stats for title\n",
    "cm, highs, _ = evaluate_recording(\n",
    "    len(signal), events, conf_map, \n",
    "    confidence_thresh=model.threshold_, \n",
    "    window_size=win_sec, tolerance=win_sec, freq=FREQ\n",
    ")\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "plot_confidence(\n",
    "    signal, conf_map, events,\n",
    "    tp, fp, tn, fn,\n",
    "    model_name=\"ECG Cost Ensemble\",\n",
    "    high_conf=highs,\n",
    "    thresh_line=model.threshold_,\n",
    "    freq=FREQ\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}